{"pages":[],"posts":[{"title":"First day here","text":"","link":"/2018/11/23/article-title/"},{"title":"EM算法原理解析","text":"EM算法是machine learning中的最经典算法之一，其重要程度我觉得不去专门记录他都感觉有点对不起对他的盛誉，倒不是因为EM算法有多么的复杂和巧妙，而是EM算法原理的应用非常广泛，在学习机器学习的过程中经常可以看到EM算法的应用，因此在blog中简单的记录一下： EM算法是一种迭代算法，是一种含有隐变量的概率模型参数的极大似然估计法，其迭代可以分成两步： E步：求期望（expectation） M步：求极大（maximization） EM算法的引入（简述）我们上面已经提到过EM算法是一个含有隐变量的概率模型参数的极大似然估计，因此我们从一个含有隐变量的问题出发： 三硬币模型： 三枚硬币A、B、C，正面概率分别为$\\pi、p、q$，先抛A，如果正面则抛B，否则C，然后根据最后抛的硬币结果，正面记为1，反面记为0，独立重复n次实验 我们用数学公式来表示三硬币模型： $P(y|\\theta) = \\sum_{z}P(y,z|\\theta) = \\sum_{z}P(z|\\theta)P(y|z,\\theta)$ 在这个公式中y对应的实际上是最后的结果，是观测变量，z对应的是隐变量，即为观测到的A的结果，$\\theta = (\\pi,p,q)$是模型参数，是根据数据的生成模型 我们将这个想法引出这个问题$Y = (Y_1,Y_2,…,Y_n)_T$，$Z = (Z_1,Z_2,…,Z_n)^T$，则似然函数为： $P(Y | \\theta) = \\sum_{Z}P(X|\\theta)P(Y|Z,\\theta)$ $\\Rightarrow P(Y|\\theta) = \\prod_{j = 1}^n [\\pi p^{y_i}(1-p)^{1-y_i} + (1-\\pi)q^{y_i}(1-q)^{1-y_i}]$ （式1） $\\hat{\\theta} = arg \\max_{\\theta} log P(Y|\\theta)$ 这个问题是没有解析解的，我们可以通过迭代来求得，即首先初始化$\\theta^{(0)}$，在E步计算$y_i$来自B的概率，即上面式一的前半部分得到概率$\\mu$，M步用$\\mu $、$y$表示模型参数中的$\\pi p q$，最后收敛可以得到结果 但是这样我们根据结果可以看到，EM算法与初值的选择是有关的 EM算法Y为观测随机变量的数据，Z表示隐随机变量的数据，Y和Z连在一起成为完全数据，Y又称为不完全数据，在这里我们先简单的声明一点：Y和Z的联合概率分布是$P(Y,Z|\\theta)$，之所以在这里强调一下，是因为在学习的过程中，关于联合概率分布和即将出现的Q函数是我的遇到的最大的坑，其根本原因就是推导学习的顺序不对。 EM算法定义： 观测变量数据Y，隐变量数据Z，输出$\\theta$ 选择$\\theta^{(0)}$，开始进行E步迭代，在i+1次迭代的E步，计算： $Q(\\theta,\\theta^{(i)}) = \\sum_{Z}logP(Y,Z|\\theta)P(Z|Y,\\theta^{(i)}$,其中$P(Z|Y,\\theta^{(i)})$是在给定观测数据Y和当前参数估计下的隐变量数据的条件概率分布 接下来进行M步，求使Q极大化的$\\theta$，$\\theta^{(i+1)} = arg \\max_{\\theta}Q(\\theta,\\theta^{(i)})$，重复迭代，直到收敛 下面开始EM算法中最关键的函数即EM算法的核心——Q函数： 完全数据的对数似然函数$logP(Y,Z|\\theta)$关于在给定观测数据Y和当前参数$\\theta^{(i)}$下对为观测数据Z的条件概率分布$P(Z|Y,\\theta^(i))$的期望称为Q函数： $Q(\\theta,\\theta^{(i)} = E_{z}[logP(Y,Z|\\theta)|Y,\\theta^{(i)}]$ 由此我们就可以知道，我们如果想要得到最终的参数那么我们就要在E步求出来Q函数的表达，然后在M步使得Q函数极大化收敛，很明显，Q函数的表达求解变成了EM算法中最重要的一步，至于收敛条件，我们可以根据参数的差或者Q函数的差都可以。 学习到这一步，我就产生了我最大的困惑：Q函数到底是怎么得到的？ 我尝试着根据Q函数的定义去理解Q函数的两个部分又或者根据逻辑关系去理解，无奈自己能力有限，实在不行，但是继续向后学习，我才发现，Q函数实际上是根据后面的推导过程不断的求出来的，没必要非要理解其中的逻辑关系，而是从公式入手，一步步的得到Q函数（后面也会发现\u001c，Q函数实际上就是我们在进行M步骤过程中迭代公式极大化去掉常数项简化得到的一个公式），那下面我们就开始进行EM算法的导出： EM算法推导 首先：$L(\\theta) = log(sum_{Z}P(Y|Z,\\theta)P(Z|\\theta))$ 上面这个公式在问题含义上和数学意义上我们都能理解 我们在M步的时候希望能够迭代逐步近似极大化$L(\\theta)$的，那么很明显，如果要迭代增大， 那么第i+1次和第i次的关系就是大于的关系，因此我们开始考虑这两次迭代的差，现在我们一直第i次，求第i+1次的，已知的数据我们才用下标： $L(\\theta)-L(\\theta^{(i)}) = log(\\sum_ZP(Y|Z,\\theta)P(Z|\\theta)) - logP(Y|\\theta^{(i)})$ 我们在这里需要利用Jensen不等式，简单的介绍一下： Jensen inequality: 如果$f$是凸函数，X是随机变量，那么以下不等式成立： $E[f(X)] \\ge f(EX)$ 具体证明过程省略，可以根据数学归纳法进行证明，网络上关于jensen不等式的一个图像也非常的生动形象 利用Jensen不等式： $L(\\theta) - L(\\theta^{(i)}) = log(\\sum_ZP(Z|Y,\\theta^{(i)})\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})}) - logP(Y|\\theta^{(i)})$ $\\ge \\sum_ZP(Z|Y,\\theta^{(i)})log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})} - logP(Y|\\theta^{(i)})$ $ = \\sum_ZP(Z|Y,\\theta^{(i)})log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})P(Y|\\theta^{(i)})}$ 至此，我们通过Jensen不等式可以求出来差的最小值，那么如果我们吧第i次迭代的结果加上去会怎么样呢？那肯定就是第i+1次的下界！ 我们定义$B(\\theta,\\theta^{(i)}) = L(\\theta^{(i)})+\\sum_ZP(Z|Y,\\theta^{(i)})log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})P(Y|\\theta^{(i)})}$ 那么$L(\\theta^{(i)},\\theta^{(i)}) = B(\\theta^{(i)},\\theta^{(i)})$，$L(\\theta) \\ge B(\\theta,\\theta^{(i)})$ 到了这一步，我们能很明显看出如果要让第i+1次最大，那么我们就要让下界最大，即$\\theta^{(i+1)} = arg\\max_{\\theta}B(\\theta,\\theta^{(i)})$ $\\theta^{(i+1)} = arg\\max_Z(L(\\theta^{(i)})+\\sum_ZP(Z|Y,\\theta^{(i)})log\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})P(Y|\\theta^{(i)})})$ 省去对于$\\theta$来说是常数的项，因为这些除了增大计算难度之外毫无用处，即去掉$\\theta^{(i)}$的项： $= arg\\max_{\\theta}(\\sum_ZP(Z|Y,\\theta^{(i)})log{P(Y|Z,\\theta)P(Z|\\theta)})$ $= arg\\max_{\\theta}(\\sum_ZP(Z|Y,\\theta^{(i)})log{P(Y,Z|\\theta)})$ 这个通过贝叶斯公式就可以求出来 $ = arg\\max_{\\theta} Q(\\theta,\\theta^{(i)})$ 至此，我们就完整的推导出了Q函数，也发现了EM算法是不断的求解下界的极大化逼近求解对数似然函数极大化的算法 到这一步，我们可能还会针对EM算法的收敛性产生疑问，EM算法是否收敛到全局最大值或局部极大值？下面引入EM算法收敛性的两个定理： EM算法收敛性 定理一：设$P(Y|\\theta)$为观测数据的似然函数，$\\theta^{(i)}(i=1,2,…)$为EM算法得到的参数估计序列，$P(Y|\\theta^{(i)})(i=1,2,…)$为对应的似然函数序列，则$P(Y|\\theta^{(i)})$是单调递增的，即 $P(Y|\\theta^{(i+1)}) \\ge P(Y|\\theta^{(i)})$ 证明上述定理： $P(Y|\\theta) = \\frac{P(Y,Z|\\theta)}{P(X|Y,\\theta)}$，这一个公式可以通过贝叶斯来证明 $logP(Y|\\theta) = logP(Y,Z|\\theta) - logP(Z|Y,\\theta)$ 已知：$Q(\\theta,\\theta^{(i)}) = \\sum_{Z}logP(Y,Z|\\theta)P(Z|Y,\\theta^{(i)})$ 令：$H(\\theta,\\theta^{(i)}) = \\sum_ZlogP(Z|Y,\\theta)P(Z|Y,\\theta^{(i)})$ $logP(Y|\\theta) = Q(\\theta,\\theta^{(i)}) - H(\\theta,\\theta^{(i)})$ $logP(Y|\\theta^{(i+1)}) - logP(Y|\\theta^{(i)})$ $=[Q(\\theta^{(i+1)},\\theta^{(i)}) - Q(\\theta^{(i)},\\theta^{(i)})] - [H(\\theta^{(i+1)},\\theta^{(i)}) - H(\\theta^{(i)},\\theta^{(i)})]$ 在这个式子中，我们只需要证明右边部分是非负的即可 首先，$Q(\\theta^{(i+1)},\\theta^{(i)}) - Q(\\theta^{(i)},\\theta^{(i)}) \\ge 0$ 只需看 $H(\\theta^{(i+1)},\\theta^{(i)}) - H(\\theta^{(i)},\\theta^{(i)})$ $=\\sum_Z(log\\frac{P(Z|Y,\\theta^{(i+1)})}{P(Z|Y,\\theta^{(i)})})P(Z|Y,\\theta^{(i)})$ $\\le log(log\\frac{P(Z|Y,\\theta^{(i+1)})}{P(Z|Y,\\theta^{(i)})}P(Z|Y,\\theta^{(i)}))$ $=log(\\sum_ZP(Z|Y,\\theta^{(i+1)})) = 0$ 因此到这里我们已经证明完了定理一是非负的 定理二：设$L(\\theta) = logP(Y|\\theta)$为观测数据的对数似然函数，$\\theta^{(i)}$为EM算法得到的参数估计序列，$L(\\theta^{(i)})(i=1,2,…)$为对应的对数似然函数序列 (1) 如果$P(Y|\\theta)$有上界，则$L(\\theta^{(i)}) = logP(Y|\\theta^{(i)})$收敛到某一值$L^{\\star}$; (2) 在函数$Q(\\theta,\\theta’)$与$\\L(\\theta)$满足一定条件下，由EM算法得到的参数估计序列$\\theta^{(i)}$的收敛值$\\theta^{\\star}$是$L(\\theta)$的稳定点 其中（1）是根据单调性和有界性得到的 代码实现：下面我直接粘贴了一下我参考的网上的最简单的EM算法实现GMM的python代码，代码比较简单，但是值得一提的是line44 分母部位应该上了标准差，我会在后期查证并修改。 GMM相关的知识点在李航的《统计学习方法》中讲的非常清楚，关于推导也非常清晰，这里粘贴我看过的比较好的关于GMM与EM写的比较好的博客链接： https://www.cnblogs.com/frontyou/p/8107883.html https://blog.csdn.net/jinping_shi/article/details/59613054 https://blog.csdn.net/sinat_22594309/article/details/65629407 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110#!/usr/bin/python# -*- coding: utf-8 -*-from __future__ import print_functionimport numpy as npdef generateData(k,mu,sigma,dataNum): ''' 产生混合高斯模型的数据 :param k: 比例系数 :param mu: 均值 :param sigma: 标准差 :param dataNum:数据个数 :return: 生成的数据 ''' # 初始化数据 dataArray = np.zeros(dataNum,dtype=np.float32) # 逐个依据概率产生数据 # 高斯分布个数 n = len(k) for i in range(dataNum): # 产生[0,1]之间的随机数 rand = np.random.random() Sum = 0 index = 0 while(index &lt; n): Sum += k[index] if(rand &lt; Sum): dataArray[i] = np.random.normal(mu[index],sigma[index]) break else: index += 1 return dataArraydef normPdf(x,mu,sigma): ''' 计算均值为mu，标准差为sigma的正态分布函数的密度函数值 :param x: x值 :param mu: 均值 :param sigma: 标准差 :return: x处的密度函数值 ''' return (1./np.sqrt(2*np.pi))*(np.exp(-(x-mu)**2/(2*sigma**2)))def em(dataArray,k,mu,sigma,step = 10): ''' em算法估计高斯混合模型 :param dataNum: 已知数据个数 :param k: 每个高斯分布的估计系数 :param mu: 每个高斯分布的估计均值 :param sigma: 每个高斯分布的估计标准差 :param step:迭代次数 :return: em 估计迭代结束估计的参数值[k,mu,sigma] ''' # 高斯分布个数 n = len(k) # 数据个数 dataNum = dataArray.size # 初始化gama数组 gamaArray = np.zeros((n,dataNum)) for s in range(step): for i in range(n): for j in range(dataNum): Sum = sum([k[t]*normPdf(dataArray[j],mu[t],sigma[t]) for t in range(n)]) gamaArray[i][j] = k[i]*normPdf(dataArray[j],mu[i],sigma[i])/float(Sum) # 更新 mu for i in range(n): mu[i] = np.sum(gamaArray[i]*dataArray)/np.sum(gamaArray[i]) # 更新 sigma for i in range(n): sigma[i] = np.sqrt(np.sum(gamaArray[i]*(dataArray - mu[i])**2)/np.sum(gamaArray[i])) # 更新系数k for i in range(n): k[i] = np.sum(gamaArray[i])/dataNum return [k,mu,sigma]if __name__ == '__main__': # 参数的准确值 k = [0.3,0.4,0.3] mu = [2,4,3] sigma = [1,1,4] # 样本数 dataNum = 5000 # 产生数据 dataArray = generateData(k,mu,sigma,dataNum) # 参数的初始值 # 注意em算法对于参数的初始值是十分敏感的 k0 = [0.3,0.3,0.4] mu0 = [1,2,2] sigma0 = [1,1,1] step = 6 # 使用em算法估计参数 k1,mu1,sigma1 = em(dataArray,k0,mu0,sigma0,step) # 输出参数的值 print(\"参数实际值:\") print(\"k:\",k) print(\"mu:\",mu) print(\"sigma:\",sigma) print(\"参数估计值:\") print(\"k1:\",k1) print(\"mu1:\",mu1) print(\"sigma1:\",sigma1)","link":"/2018/12/17/EM算法原理解析/"},{"title":"PCA整理","text":"在PCA学习上遇到了很多的困难，总结如下: 线性代数知识点的模糊 高等数学知识的遗忘 过分依赖空间想象 其中，我想再开始解释第三点“过分依赖空间想象”，我们在学习的过程中总是不自觉的将线性变换通过二维空间或者三维空间中想象来进行解释，这种做法固然好，可以很直观地帮助我们理解数学问题，但是在多维空间中我们要通过二维到多维，正视矩阵！矩阵、向量这些概念帮助我们解释了多维空间里的变化，因此我们要通过矩阵看问题，不要通过空间看矩阵。 因此特地将PCA原理及python实现整理下来，以便后面可以及时\b回顾复习。 在本文实际上提供了两种理解思路，在简介里的思路是通过拉格朗日乘数法进行解释，在下面问题里主要通过矩阵的形式进行解释。 \bpca（principal component analysis）是一种降维技术，将多维数据在损失比较小的情况下转换到低维，\b所谓损失比较小就是我们将一些计算价值量较低（variance较小）的维度清除掉，保留差异性较大的维度，这些维度才是值得我们去考虑并且计算的部分，pca方法和其他降维方法一样，都是为了减轻计算量。 $z=\\omega x$ Input : x output : z（在unsupervised learning中不知道z的样子，是根据大量的x来寻找$\\omega$的）我们要使得选出的$\\omega$让z的variance越大越好，以此来消除歧义（PCA解释方差对离群点非常敏感），否则variance近似，我们的数据是没有意义的。 简单介绍PCA的原理首先根据公式我们知道我们的目标是找$\\omega$来 首先我们先从1-dim空间中进行解决，假设$\\omega$是一维的，我们发现z和x的关系实际上就是x到$\\omega$的投影，在这里我们假设$\\left|\\omega\\right|2=1$，之所以希望norm为1，是因为我们希望看到的是$x$到$z$的投影是$x$的长度，那么我们知道inner product里$x \\cdot w$的结果是$\\left|w\\right|\\left|x\\right|\\cos\\theta$,我们只有让$w$的norm为1才才能够做到这一点。如果寻找这个$w$，我们的loss函数为$var(z) = \\sum{z}(z-\\overline{z})^2$最大即可 当在2-dim空间中，我们需要讨论的不仅仅是$\\left|w_2\\right|_2=$了，而是要考虑$w_1$和$w_2$之间的关系，如果我们保持原先的variance最大，那么$w_1$和$w_2$就是一样的，所以我们要让他们正交,即$w_1 \\cdot w_2 = 0$，其实： $\\begin{pmatrix} w_1 \\ w_2 \\ w_3 \\ …\\end{pmatrix}$是orthogonal matrix(正交矩阵) 接下来我们对$var(z) = \\sum_{z}(z-\\bar{z})^2$ 利用lagrange进行求解： $var(z) = \\sum_{z}(z-\\bar{z})^2 = \\sum(w \\cdot(x-\\bar{x}))^2$ 将$w$看作$a$，将$(x-\\bar{x})$看作$b$: $(a \\cdot b)^2 = (a^Tb)^2 = a^Tba^Tb = a^Tb(a^Tb)^T = a^Tbb^Ta$ 因此$var(z) = (w)^T\\sum(x-\\bar{x})(x-\\bar{x})^T (w) = (w)^T cov(x) (w)$ $S = cov(x)$，接下来我们要做的就是find $w$ 使得$(w)^TS(w)$最大 我们可以通过Lagrange multiplier进行求解： $g(w) = (w)^TS (w) - \\alpha((w)^Tw)- 1)$ $\\Rightarrow Sw-\\alpha w = 0$ 由此我们看出要想满足最大效果，$w$为$S$的eigen vecotr,$w$为最大的eigen value 由此我们基本解决了$w$在1-dim中的求解，在2-dim中我们怎么解决呢，多了一个变量！多一个函数即可解决 $\\Rightarrow (w_1)^Tw_2 = 0$ 我们可以因此得出$\\beta = 0$，我们也可以得到$\\beta$对应$S$中的第二大eigen value $\\lambda_2$ 至此我们基本解决了PCA的原理，我们接下来就是对应实践的过程，在实践中，我们发现我们有很多的疑问，我将我的疑问进行了以下整理 在PCA中我们需要解释的几件事情： 线性变换的意义 协方差矩阵的意义 为什么处理数据时要同时减去均值 Lagrange multiplier 矩阵对角化 为什么要线性变换？ 将N维向量转换为K维向量时（$N &gt; K$），目标是选择K个单位正交基，使得原始数据变换到这组基上，这些基的协方差为0目的是字段的方差尽可能大 为什么要通过协方差矩阵？ 协方差是用来刻画两个随机变量之间的相关性，反映变量之间的二阶统计特性 $cov(a,b) = \\frac{1}{m}\\sum_{i=1}^m(a_i-\\mu_a)(b_i-\\mu_b)$ 随机变量$X_i X_j$ $cov(X_i,X_j) = E[(X_i-E(X_i))(X_j-E(X_j))]$ 对于数据集矩阵: $X = \\begin{pmatrix}a_1&amp;a_2&amp;…&amp;a_m\\b_1&amp;b_2&amp;…&amp;b_m \\end{pmatrix}$，特征维度为2 假设均值为0，我们可以通过$\\frac{1}{m}XX^T$得到协方差矩阵： $\\begin{pmatrix}\\frac{1}{m}\\sum_{i=1}^m a_i^2 &amp;\\frac{1}{m}\\sum_{i=1}^m a_ib_i \\ \\frac{1}{m}\\sum_{i=1}^ma_ib_i &amp; \\frac{1}{m}\\sum_{i=1}^m b_i^2 \\end{pmatrix}$ 由矩阵可以知道，对角线上的元素就是均值为0情况下的方差，而非对角线上的元素就是两个字段的协方差，所以协方差矩阵直接满足了我们所需要的两个条件，下面我们只需要对方差最大、协方差最小来进行就可以了 如何让Max和min同时满足呢，我们要做的就是协方差矩阵对角化（除对角线外的元素化为0，对角线元素从大到小排列） 我们假设output为Y，input为X，分别对应协方差矩阵$Y_c$$X_c$ $Y_c = \\frac{1}{m}YY^T = \\frac{1}{m}(PX)(PX)^T = \\frac{1}{m}PXX^TP^T = P(\\frac{1}{m}XX^T)P^T = PX_cP^T$ 由此我们看到Y和X的协方差之间的关系，我们要求的Y协方差矩阵对角化，那么我们就要让$PX_cP^T$为对角矩阵，在这里P实际上就是$X_c$对应的特征向量组成的矩阵 为什么处理数据要减去均值？ 在上面的步骤中其实已经解决了这个问题，目的就是减去均值后相当于这群数据的均值为0，那么协方差矩阵对角线元素直接对应了他们的方差，并且这也是归一化的步骤，防止不同元素数值差距太大 拉格朗日算子 矩阵对角化作用 之所以要矩阵最小化，目的是为了利用他的数学性质，首先实对称矩阵不同特征值对应特征向量正交，其次特征值重数对应重数个特征向量，可以对这些特征向量单位正交化 1234567891011121314151617181920212223242526272829import numpy as npimport pandas as pdimport matplotlib.pyplot as pltdef meanX(dataX): return np.mean(dataX,axis=0)def pca(XMat,k): average = meanX(XMat) m,n = np.shape(XMat) data_adjust = [] avgs = np.tile(average,(m,1)) data_adjust = XMat - avgs covX = np.cov(data_adjust.T) featValue,featVec = np.linalg.eig(covX) index = np.argsort(-featValue) finalData = [] if k &gt; n: print \"k must lower than feature number\" return else: selectVec = np.matrix(featVec[index[:k]]) finalData = data_adjust*selectVec.T reconData = (finalData*selectVec) + average return finalData,reconDatadef loaddata(datafile): return np.array(pd.read_csv(datafile,sep=\"\\t\",header=-1)).astype(np.float)","link":"/2018/12/06/PCA/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/11/22/hello-world/"}],"tags":[{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"}],"categories":[]}