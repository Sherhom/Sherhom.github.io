{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/11/22/hello-world/"},{"title":"First day here","text":"","link":"/2018/11/23/article-title/"},{"title":"PCA整理","text":"在PCA学习上遇到了很多的困难，总结如下: 线性代数知识点的模糊 高等数学知识的遗忘 过分依赖空间想象 其中，我想再开始解释第三点“过分依赖空间想象”，我们在学习的过程中总是不自觉的将线性变换通过二维空间或者三维空间中想象来进行解释，这种做法固然好，可以很直观地帮助我们理解数学问题，但是在多维空间中我们要通过二维到多维，正视矩阵！矩阵、向量这些概念帮助我们解释了多维空间里的变化，因此我们要通过矩阵看问题，不要通过空间看矩阵。 因此特地将PCA原理及python实现整理下来，以便后面可以及时\b回顾复习。 在本文实际上提供了两种理解思路，在简介里的思路是通过拉格朗日乘数法进行解释，在下面问题里主要通过矩阵的形式进行解释。 \bpca（principal component analysis）是一种降维技术，将多维数据在损失比较小的情况下转换到低维，\b所谓损失比较小就是我们将一些计算价值量较低（variance较小）的维度清除掉，保留差异性较大的维度，这些维度才是值得我们去考虑并且计算的部分，pca方法和其他降维方法一样，都是为了减轻计算量。 $z=\\omega x$ Input : x output : z（在unsupervised learning中不知道z的样子，是根据大量的x来寻找$\\omega$的）我们要使得选出的$\\omega$让z的variance越大越好，以此来消除歧义（PCA解释方差对离群点非常敏感），否则variance近似，我们的数据是没有意义的。 简单介绍PCA的原理首先根据公式我们知道我们的目标是找$\\omega$来 首先我们先从1-dim空间中进行解决，假设$\\omega$是一维的，我们发现z和x的关系实际上就是x到$\\omega$的投影，在这里我们假设$\\left|\\omega\\right|2=1$，之所以希望norm为1，是因为我们希望看到的是$x$到$z$的投影是$x$的长度，那么我们知道inner product里$x \\cdot w$的结果是$\\left|w\\right|\\left|x\\right|\\cos\\theta$,我们只有让$w$的norm为1才才能够做到这一点。如果寻找这个$w$，我们的loss函数为$var(z) = \\sum{z}(z-\\overline{z})^2$最大即可 当在2-dim空间中，我们需要讨论的不仅仅是$\\left|w_2\\right|_2=$了，而是要考虑$w_1$和$w_2$之间的关系，如果我们保持原先的variance最大，那么$w_1$和$w_2$就是一样的，所以我们要让他们正交,即$w_1 \\cdot w_2 = 0$，其实： $\\begin{pmatrix} w_1 \\ w_2 \\ w_3 \\ …\\end{pmatrix}$是orthogonal matrix(正交矩阵) 接下来我们对$var(z) = \\sum_{z}(z-\\bar{z})^2$ 利用lagrange进行求解： $var(z) = \\sum_{z}(z-\\bar{z})^2 = \\sum(w \\cdot(x-\\bar{x}))^2$ 将$w$看作$a$，将$(x-\\bar{x})$看作$b$: $(a \\cdot b)^2 = (a^Tb)^2 = a^Tba^Tb = a^Tb(a^Tb)^T = a^Tbb^Ta$ 因此$var(z) = (w)^T\\sum(x-\\bar{x})(x-\\bar{x})^T (w) = (w)^T cov(x) (w)$ $S = cov(x)$，接下来我们要做的就是find $w$ 使得$(w)^TS(w)$最大 我们可以通过Lagrange multiplier进行求解： $g(w) = (w)^TS (w) - \\alpha((w)^Tw)- 1)$ $\\Rightarrow Sw-\\alpha w = 0$ 由此我们看出要想满足最大效果，$w$为$S$的eigen vecotr,$w$为最大的eigen value 由此我们基本解决了$w$在1-dim中的求解，在2-dim中我们怎么解决呢，多了一个变量！多一个函数即可解决 $\\Rightarrow (w_1)^Tw_2 = 0$ 我们可以因此得出$\\beta = 0$，我们也可以得到$\\beta$对应$S$中的第二大eigen value $\\lambda_2$ 至此我们基本解决了PCA的原理，我们接下来就是对应实践的过程，在实践中，我们发现我们有很多的疑问，我将我的疑问进行了以下整理 在PCA中我们需要解释的几件事情： 线性变换的意义 协方差矩阵的意义 为什么处理数据时要同时减去均值 Lagrange multiplier 矩阵对角化 为什么要线性变换？ 将N维向量转换为K维向量时（$N &gt; K$），目标是选择K个单位正交基，使得原始数据变换到这组基上，这些基的协方差为0目的是字段的方差尽可能大 为什么要通过协方差矩阵？ 协方差是用来刻画两个随机变量之间的相关性，反映变量之间的二阶统计特性 $cov(a,b) = \\frac{1}{m}\\sum_{i=1}^m(a_i-\\mu_a)(b_i-\\mu_b)$ 随机变量$X_i X_j$ $cov(X_i,X_j) = E[(X_i-E(X_i))(X_j-E(X_j))]$ 对于数据集矩阵: $X = \\begin{pmatrix}a_1&amp;a_2&amp;…&amp;a_m\\b_1&amp;b_2&amp;…&amp;b_m \\end{pmatrix}$，特征维度为2 假设均值为0，我们可以通过$\\frac{1}{m}XX^T$得到协方差矩阵： $\\begin{pmatrix}\\frac{1}{m}\\sum_{i=1}^m a_i^2 &amp;\\frac{1}{m}\\sum_{i=1}^m a_ib_i \\ \\frac{1}{m}\\sum_{i=1}^ma_ib_i &amp; \\frac{1}{m}\\sum_{i=1}^m b_i^2 \\end{pmatrix}$ 由矩阵可以知道，对角线上的元素就是均值为0情况下的方差，而非对角线上的元素就是两个字段的协方差，所以协方差矩阵直接满足了我们所需要的两个条件，下面我们只需要对方差最大、协方差最小来进行就可以了 如何让Max和min同时满足呢，我们要做的就是协方差矩阵对角化（除对角线外的元素化为0，对角线元素从大到小排列） 我们假设output为Y，input为X，分别对应协方差矩阵$Y_c$$X_c$ $Y_c = \\frac{1}{m}YY^T = \\frac{1}{m}(PX)(PX)^T = \\frac{1}{m}PXX^TP^T = P(\\frac{1}{m}XX^T)P^T = PX_cP^T$ 由此我们看到Y和X的协方差之间的关系，我们要求的Y协方差矩阵对角化，那么我们就要让$PX_cP^T$为对角矩阵，在这里P实际上就是$X_c$对应的特征向量组成的矩阵 为什么处理数据要减去均值？ 在上面的步骤中其实已经解决了这个问题，目的就是减去均值后相当于这群数据的均值为0，那么协方差矩阵对角线元素直接对应了他们的方差，并且这也是归一化的步骤，防止不同元素数值差距太大 拉格朗日算子 矩阵对角化作用 之所以要矩阵最小化，目的是为了利用他的数学性质，首先实对称矩阵不同特征值对应特征向量正交，其次特征值重数对应重数个特征向量，可以对这些特征向量单位正交化 1234567891011121314151617181920212223242526272829import numpy as npimport pandas as pdimport matplotlib.pyplot as pltdef meanX(dataX): return np.mean(dataX,axis=0)def pca(XMat,k): average = meanX(XMat) m,n = np.shape(XMat) data_adjust = [] avgs = np.tile(average,(m,1)) data_adjust = XMat - avgs covX = np.cov(data_adjust.T) featValue,featVec = np.linalg.eig(covX) index = np.argsort(-featValue) finalData = [] if k &gt; n: print \"k must lower than feature number\" return else: selectVec = np.matrix(featVec[index[:k]]) finalData = data_adjust*selectVec.T reconData = (finalData*selectVec) + average return finalData,reconDatadef loaddata(datafile): return np.array(pd.read_csv(datafile,sep=\"\\t\",header=-1)).astype(np.float)","link":"/2018/12/06/PCA/"}],"tags":[{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"}],"categories":[]}