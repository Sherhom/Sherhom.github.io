<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>SherhomYe</title><meta name="author" content="Fei Ye"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">SherhomYe</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/#Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/"> About</a></li><li class="menus_item"><a class="site-page" href="https://phower.me"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope="" itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Fei Ye</h3><p class="author-bio">Your biography can be writed down here.</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope="" itemtype="http://schema.org/CreativeWork"><h2 class="page-title">EM算法原理解析</h2><article><p>EM算法是machine learning中的最经典算法之一，其重要程度我觉得不去专门记录他都感觉有点对不起对他的盛誉，倒不是因为EM算法有多么的复杂和巧妙，而是EM算法原理的应用非常广泛，在学习机器学习的过程中经常可以看到EM算法的应用，因此在blog中简单的记录一下：</p>
<blockquote>
<p>EM算法是一种迭代算法，是一种含有隐变量的概率模型参数的极大似然估计法，其迭代可以分成两步：</p>
<p>E步：求期望（expectation）</p>
<p>M步：求极大（maximization）</p>
</blockquote>
<hr>
<h4 id="EM算法的引入（简述）"><a href="#EM算法的引入（简述）" class="headerlink" title="EM算法的引入（简述）"></a>EM算法的引入（简述）</h4><p>我们上面已经提到过EM算法是一个<strong>含有隐变量</strong>的概率模型参数的极大似然估计，因此我们从一个含有隐变量的问题出发：</p>
<blockquote>
<p>三硬币模型：</p>
<p>三枚硬币A、B、C，正面概率分别为$\pi、p、q$，先抛A，如果正面则抛B，否则C，然后根据最后抛的硬币结果，正面记为1，反面记为0，独立重复n次实验</p>
</blockquote>
<p>我们用数学公式来表示三硬币模型：</p>
<p>$P(y|\theta) = \sum_{z}P(y,z|\theta) = \sum_{z}P(z|\theta)P(y|z,\theta)$</p>
<p>在这个公式中y对应的实际上是最后的结果，是观测变量，z对应的是隐变量，即为观测到的A的结果，$\theta = (\pi,p,q)$是模型参数，是根据数据的生成模型</p>
<p>我们将这个想法引出这个问题$Y = (Y_1,Y_2,…,Y_n)_T$，$Z = (Z_1,Z_2,…,Z_n)^T$，则似然函数为：</p>
<p>$P(Y | \theta) = \sum_{Z}P(X|\theta)P(Y|Z,\theta)$</p>
<p>$\Rightarrow P(Y|\theta) = \prod_{j = 1}^n [\pi p^{y_i}(1-p)^{1-y_i}  + (1-\pi)q^{y_i}(1-q)^{1-y_i}]$            （式1）</p>
<p>$\hat{\theta} = arg \max_{\theta} log P(Y|\theta)$</p>
<p>这个问题是没有解析解的，我们可以通过迭代来求得，即首先初始化$\theta^{(0)}$，在E步计算$y_i$来自B的概率，即上面式一的前半部分得到概率$\mu$，M步用$\mu $、$y$表示模型参数中的$\pi p q$，最后收敛可以得到结果</p>
<p>但是这样我们根据结果可以看到，EM算法与初值的选择是有关的</p>
<hr>
<h4 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h4><p>Y为观测随机变量的数据，Z表示隐随机变量的数据，Y和Z连在一起成为完全数据，Y又称为不完全数据，在这里我们先简单的声明一点：Y和Z的联合概率分布是$P(Y,Z|\theta)$，之所以在这里强调一下，是因为在学习的过程中，关于联合概率分布和即将出现的Q函数是我的遇到的最大的坑，其根本原因就是推导学习的顺序不对。</p>
<p><strong>EM算法定义</strong>：</p>
<p>观测变量数据Y，隐变量数据Z，输出$\theta$</p>
<p>选择$\theta^{(0)}$，开始进行E步迭代，在i+1次迭代的E步，计算：</p>
<p>$Q(\theta,\theta^{(i)}) = \sum_{Z}logP(Y,Z|\theta)P(Z|Y,\theta^{(i)}$,其中$P(Z|Y,\theta^{(i)})$是在给定观测数据Y和当前参数估计下的隐变量数据的条件概率分布</p>
<p>接下来进行M步，求使Q极大化的$\theta$，$\theta^{(i+1)} = arg \max_{\theta}Q(\theta,\theta^{(i)})$，重复迭代，直到收敛</p>
<p>下面开始EM算法中最关键的函数即EM算法的核心——Q函数：</p>
<blockquote>
<p>完全数据的对数似然函数$logP(Y,Z|\theta)$关于在给定观测数据Y和当前参数$\theta^{(i)}$下对为观测数据Z的条件概率分布$P(Z|Y,\theta^(i))$的期望称为Q函数：</p>
<p>$Q(\theta,\theta^{(i)} = E_{z}[logP(Y,Z|\theta)|Y,\theta^{(i)}]$</p>
</blockquote>
<p>由此我们就可以知道，我们如果想要得到最终的参数那么我们就要在E步求出来Q函数的表达，然后在M步使得Q函数极大化收敛，很明显，Q函数的表达求解变成了EM算法中最重要的一步，至于收敛条件，我们可以根据参数的差或者Q函数的差都可以。</p>
<p>学习到这一步，我就产生了我最大的困惑：<strong>Q函数到底是怎么得到的？</strong></p>
<p>我尝试着根据Q函数的定义去理解Q函数的两个部分又或者根据逻辑关系去理解，无奈自己能力有限，实在不行，但是继续向后学习，我才发现，Q函数实际上是根据后面的推导过程不断的求出来的，没必要非要理解其中的逻辑关系，而是从公式入手，一步步的得到Q函数（后面也会发现，Q函数实际上就是我们在进行M步骤过程中迭代公式极大化去掉常数项简化得到的一个公式），那下面我们就开始进行EM算法的导出：</p>
<p><strong>EM算法推导</strong></p>
<p>首先：$L(\theta) = log(sum_{Z}P(Y|Z,\theta)P(Z|\theta))$</p>
<p>上面这个公式在问题含义上和数学意义上我们都能理解</p>
<p>我们在M步的时候希望能够迭代逐步近似极大化$L(\theta)$的，那么很明显，如果要迭代增大， 那么第i+1次和第i次的关系就是大于的关系，因此我们开始考虑这两次迭代的差，现在我们一直第i次，求第i+1次的，已知的数据我们才用下标：</p>
<p>$L(\theta)-L(\theta^{(i)}) = log(\sum_ZP(Y|Z,\theta)P(Z|\theta)) - logP(Y|\theta^{(i)})$</p>
<p>我们在这里需要利用Jensen不等式，简单的介绍一下：</p>
<blockquote>
<p>Jensen inequality:</p>
<p>如果$f$是凸函数，X是随机变量，那么以下不等式成立：</p>
<p>$E[f(X)] \ge f(EX)$</p>
<p>具体证明过程省略，可以根据数学归纳法进行证明，网络上关于jensen不等式的一个图像也非常的生动形象</p>
</blockquote>
<p>利用Jensen不等式：</p>
<p>$L(\theta) - L(\theta^{(i)}) = log(\sum_ZP(Z|Y,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}) - logP(Y|\theta^{(i)})$</p>
<p>$\ge \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})} - logP(Y|\theta^{(i)})$</p>
<p>$ = \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$</p>
<p>至此，我们通过Jensen不等式可以求出来差的最小值，那么如果我们吧第i次迭代的结果加上去会怎么样呢？那肯定就是第i+1次的下界！</p>
<p>我们定义$B(\theta,\theta^{(i)}) = L(\theta^{(i)})+\sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$</p>
<p>那么$L(\theta^{(i)},\theta^{(i)}) = B(\theta^{(i)},\theta^{(i)})$，$L(\theta) \ge B(\theta,\theta^{(i)})$</p>
<p>到了这一步，我们能很明显看出如果要让第i+1次最大，那么我们就要让下界最大，即$\theta^{(i+1)} = arg\max_{\theta}B(\theta,\theta^{(i)})$</p>
<p>$\theta^{(i+1)} = arg\max_Z(L(\theta^{(i)})+\sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})})$</p>
<p>省去对于$\theta$来说是常数的项，因为这些除了增大计算难度之外毫无用处，即去掉$\theta^{(i)}$的项：</p>
<p>$= arg\max_{\theta}(\sum_ZP(Z|Y,\theta^{(i)})log{P(Y|Z,\theta)P(Z|\theta)})$</p>
<p>$= arg\max_{\theta}(\sum_ZP(Z|Y,\theta^{(i)})log{P(Y,Z|\theta)})$</p>
<p>这个通过贝叶斯公式就可以求出来</p>
<p>$ = arg\max_{\theta} Q(\theta,\theta^{(i)})$</p>
<p>至此，我们就完整的推导出了Q函数，也发现了EM算法是不断的求解下界的极大化逼近求解对数似然函数极大化的算法</p>
<p>到这一步，我们可能还会针对EM算法的收敛性产生疑问，EM算法是否收敛到全局最大值或局部极大值？下面引入EM算法收敛性的两个定理：</p>
<h4 id="EM算法收敛性"><a href="#EM算法收敛性" class="headerlink" title="EM算法收敛性"></a>EM算法收敛性</h4><blockquote>
<p>定理一：设$P(Y|\theta)$为观测数据的似然函数，$\theta^{(i)}(i=1,2,…)$为EM算法得到的参数估计序列，$P(Y|\theta^{(i)})(i=1,2,…)$为对应的似然函数序列，则$P(Y|\theta^{(i)})$是单调递增的，即</p>
<p>$P(Y|\theta^{(i+1)}) \ge P(Y|\theta^{(i)})$</p>
</blockquote>
<p>证明上述定理：</p>
<p>$P(Y|\theta) = \frac{P(Y,Z|\theta)}{P(X|Y,\theta)}$，这一个公式可以通过贝叶斯来证明</p>
<p>$logP(Y|\theta) = logP(Y,Z|\theta) - logP(Z|Y,\theta)$</p>
<p>已知：$Q(\theta,\theta^{(i)}) = \sum_{Z}logP(Y,Z|\theta)P(Z|Y,\theta^{(i)})$</p>
<p>令：$H(\theta,\theta^{(i)}) = \sum_ZlogP(Z|Y,\theta)P(Z|Y,\theta^{(i)})$</p>
<p>$logP(Y|\theta) = Q(\theta,\theta^{(i)}) - H(\theta,\theta^{(i)})$</p>
<p>$logP(Y|\theta^{(i+1)}) - logP(Y|\theta^{(i)})$</p>
<p>$=[Q(\theta^{(i+1)},\theta^{(i)}) - Q(\theta^{(i)},\theta^{(i)})] - [H(\theta^{(i+1)},\theta^{(i)}) - H(\theta^{(i)},\theta^{(i)})]$</p>
<p>在这个式子中，我们只需要证明右边部分是非负的即可</p>
<p>首先，$Q(\theta^{(i+1)},\theta^{(i)}) - Q(\theta^{(i)},\theta^{(i)}) \ge 0$</p>
<p>只需看 $H(\theta^{(i+1)},\theta^{(i)}) - H(\theta^{(i)},\theta^{(i)})$</p>
<p>$=\sum_Z(log\frac{P(Z|Y,\theta^{(i+1)})}{P(Z|Y,\theta^{(i)})})P(Z|Y,\theta^{(i)})$</p>
<p>$\le log(log\frac{P(Z|Y,\theta^{(i+1)})}{P(Z|Y,\theta^{(i)})}P(Z|Y,\theta^{(i)}))$</p>
<p>$=log(\sum_ZP(Z|Y,\theta^{(i+1)})) = 0$</p>
<p>因此到这里我们已经证明完了定理一是非负的</p>
<blockquote>
<p>定理二：设$L(\theta) = logP(Y|\theta)$为观测数据的对数似然函数，$\theta^{(i)}$为EM算法得到的参数估计序列，$L(\theta^{(i)})(i=1,2,…)$为对应的对数似然函数序列</p>
<p>(1) 如果$P(Y|\theta)$有上界，则$L(\theta^{(i)}) = logP(Y|\theta^{(i)})$收敛到某一值$L^{\star}$;</p>
<p>(2) 在函数$Q(\theta,\theta’)$与$\L(\theta)$满足一定条件下，由EM算法得到的参数估计序列$\theta^{(i)}$的收敛值$\theta^{\star}$是$L(\theta)$的稳定点</p>
</blockquote>
<p>其中（1）是根据单调性和有界性得到的</p>
<hr>
<h4 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h4><p>下面我直接粘贴了一下我参考的网上的最简单的EM算法实现GMM的python代码，代码比较简单，但是值得一提的是line44 分母部位应该上了标准差，我会在后期查证并修改。</p>
<p>GMM相关的知识点在李航的《统计学习方法》中讲的非常清楚，关于推导也非常清晰，这里粘贴我看过的比较好的关于GMM与EM写的比较好的博客链接：</p>
<p><a href="https://www.cnblogs.com/frontyou/p/8107883.html" target="_blank" rel="noopener">https://www.cnblogs.com/frontyou/p/8107883.html</a></p>
<p><a href="https://blog.csdn.net/jinping_shi/article/details/59613054" target="_blank" rel="noopener">https://blog.csdn.net/jinping_shi/article/details/59613054</a></p>
<p><a href="https://blog.csdn.net/sinat_22594309/article/details/65629407" target="_blank" rel="noopener">https://blog.csdn.net/sinat_22594309/article/details/65629407</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateData</span><span class="params">(k,mu,sigma,dataNum)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    产生混合高斯模型的数据</span></span><br><span class="line"><span class="string">    :param k: 比例系数</span></span><br><span class="line"><span class="string">    :param mu: 均值</span></span><br><span class="line"><span class="string">    :param sigma: 标准差</span></span><br><span class="line"><span class="string">    :param dataNum:数据个数</span></span><br><span class="line"><span class="string">    :return: 生成的数据</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 初始化数据</span></span><br><span class="line">    dataArray = np.zeros(dataNum,dtype=np.float32)</span><br><span class="line">    <span class="comment"># 逐个依据概率产生数据</span></span><br><span class="line">    <span class="comment"># 高斯分布个数</span></span><br><span class="line">    n = len(k)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(dataNum):</span><br><span class="line">        <span class="comment"># 产生[0,1]之间的随机数</span></span><br><span class="line">        rand = np.random.random()</span><br><span class="line">        Sum = <span class="number">0</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span>(index &lt; n):</span><br><span class="line">            Sum += k[index]</span><br><span class="line">            <span class="keyword">if</span>(rand &lt; Sum):</span><br><span class="line">                dataArray[i] = np.random.normal(mu[index],sigma[index])</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> dataArray</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normPdf</span><span class="params">(x,mu,sigma)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算均值为mu，标准差为sigma的正态分布函数的密度函数值</span></span><br><span class="line"><span class="string">    :param x: x值</span></span><br><span class="line"><span class="string">    :param mu: 均值</span></span><br><span class="line"><span class="string">    :param sigma: 标准差</span></span><br><span class="line"><span class="string">    :return: x处的密度函数值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.</span>/np.sqrt(<span class="number">2</span>*np.pi))*(np.exp(-(x-mu)**<span class="number">2</span>/(<span class="number">2</span>*sigma**<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">em</span><span class="params">(dataArray,k,mu,sigma,step = <span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    em算法估计高斯混合模型</span></span><br><span class="line"><span class="string">    :param dataNum: 已知数据个数</span></span><br><span class="line"><span class="string">    :param k: 每个高斯分布的估计系数</span></span><br><span class="line"><span class="string">    :param mu: 每个高斯分布的估计均值</span></span><br><span class="line"><span class="string">    :param sigma: 每个高斯分布的估计标准差</span></span><br><span class="line"><span class="string">    :param step:迭代次数</span></span><br><span class="line"><span class="string">    :return: em 估计迭代结束估计的参数值[k,mu,sigma]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 高斯分布个数</span></span><br><span class="line">    n = len(k)</span><br><span class="line">    <span class="comment"># 数据个数</span></span><br><span class="line">    dataNum = dataArray.size</span><br><span class="line">    <span class="comment"># 初始化gama数组</span></span><br><span class="line">    gamaArray = np.zeros((n,dataNum))</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> range(step):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(dataNum):</span><br><span class="line">                Sum = sum([k[t]*normPdf(dataArray[j],mu[t],sigma[t]) <span class="keyword">for</span> t <span class="keyword">in</span> range(n)])</span><br><span class="line">                gamaArray[i][j] = k[i]*normPdf(dataArray[j],mu[i],sigma[i])/float(Sum)</span><br><span class="line">        <span class="comment"># 更新 mu</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            mu[i] = np.sum(gamaArray[i]*dataArray)/np.sum(gamaArray[i])</span><br><span class="line">        <span class="comment"># 更新 sigma</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            sigma[i] = np.sqrt(np.sum(gamaArray[i]*(dataArray - mu[i])**<span class="number">2</span>)/np.sum(gamaArray[i]))</span><br><span class="line">        <span class="comment"># 更新系数k</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            k[i] = np.sum(gamaArray[i])/dataNum</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [k,mu,sigma]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 参数的准确值</span></span><br><span class="line">    k = [<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.3</span>]</span><br><span class="line">    mu = [<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>]</span><br><span class="line">    sigma = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>]</span><br><span class="line">    <span class="comment"># 样本数</span></span><br><span class="line">    dataNum = <span class="number">5000</span></span><br><span class="line">    <span class="comment"># 产生数据</span></span><br><span class="line">    dataArray = generateData(k,mu,sigma,dataNum)</span><br><span class="line">    <span class="comment"># 参数的初始值</span></span><br><span class="line">    <span class="comment"># 注意em算法对于参数的初始值是十分敏感的</span></span><br><span class="line">    k0 = [<span class="number">0.3</span>,<span class="number">0.3</span>,<span class="number">0.4</span>]</span><br><span class="line">    mu0 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]</span><br><span class="line">    sigma0 = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">    step = <span class="number">6</span></span><br><span class="line">    <span class="comment"># 使用em算法估计参数</span></span><br><span class="line">    k1,mu1,sigma1 = em(dataArray,k0,mu0,sigma0,step)</span><br><span class="line">    <span class="comment"># 输出参数的值</span></span><br><span class="line">    print(<span class="string">"参数实际值:"</span>)</span><br><span class="line">    print(<span class="string">"k:"</span>,k)</span><br><span class="line">    print(<span class="string">"mu:"</span>,mu)</span><br><span class="line">    print(<span class="string">"sigma:"</span>,sigma)</span><br><span class="line">    print(<span class="string">"参数估计值:"</span>)</span><br><span class="line">    print(<span class="string">"k1:"</span>,k1)</span><br><span class="line">    print(<span class="string">"mu1:"</span>,mu1)</span><br><span class="line">    print(<span class="string">"sigma1:"</span>,sigma1)</span><br></pre></td></tr></table></figure>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/#Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/"> About</a></li><li class="nav_item"><a class="nav-page" href="https://phower.me"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 by Fei Ye</div><div class="theme-info">Powered by <a href="https://hexo.io" rel="nofollow">Hexo</a> & <a href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>